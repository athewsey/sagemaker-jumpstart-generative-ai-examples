{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Prompt Engineering\n",
    "\n",
    "***\n",
    "In this lab, we will cover what is prompt engineering, what are some real world use cases, and advanced prompt engineering insights.\n",
    "***\n",
    "\n",
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Run inference on the pre-trained model](#2.-run-inference-on-the-pre-trained-model)\n",
    "3. [Query endpoint and parse response](#3.-Query-endpoint-and-parse-response)\n",
    "4. [Use Cases](#4.-Use-Cases)\n",
    "5. [Impact of parameters on performance](#5.-Impact-of-parameters-on-performance)\n",
    "6. [Clean up the endpoint](#6.-Clean-up-the-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "\n",
    "The endpoint URL is pointing to a AI21 Jurassic 2 Grande Instruct model deployed on Amazon Sagemaker endpoint. This will be the model used for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "url = \"https://e165qoav65.execute-api.us-east-1.amazonaws.com/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a method that will invoke the REST API via HTTP request to interact with the model. The returned response will be the\n",
    "# text generated by the model.\n",
    "def query_endpoint_with_json_payload(url, payload):\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=payload,\n",
    "    )\n",
    "    \n",
    "    response_body = response.content\n",
    "    response_json = json.loads(response_body.decode('utf-8'))\n",
    "    generated_text = response_json['completions'][0]['data']['text']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Parameters\n",
    "\n",
    "***\n",
    "AI21 Jurassic 2 Models supports many advanced parameters while performing inference. Here are some commonly used parameters:\n",
    "\n",
    "* **maxTokens:** The maximum number of tokens to generate per result. Optional, default = 16. If no stopSequences are given, generation is stopped after producing maxTokens\n",
    "* **numResults:** Number of completions to sample and return. Optional, default = 1\n",
    "* **temperature:** Modifies the distribution from which tokens are sampled. Optional, default = 0.7 Setting temperature to 1.0 samples directly from the model distribution. Lower (higher) values increase the chance of sampling higher (lower) probability tokens. A value of 0 essentially disables sampling and results in greedy decoding, where the most likely token is chosen at every step. â€‹\n",
    "* **topKReturn:** 0 <= integer <= 10, Optional, default = 0. Return the top-K alternative tokens. When using a non-zero value, the response includes the string representations and logprobs for each of the top-K alternatives at each position, in the prompt and in the completions.\n",
    "* **topP:** Sample tokens from the corresponding top percentile of probability mass. Optional, default = 1. For example, a value of 0.9 will only consider tokens comprising the top 90% probability mass.\n",
    "* **stopSequences:** Stops decoding if any of the strings is generated. Optional. For example, to stop at a comma or a new line use [\".\", \"\\n\"]. The decoded result text will not include the stop sequence string, but it will be included in the raw token data, which can also continue beyond the stop sequence if the sequence ended in the middle of a token. The sequence which triggered the termination will be included in the finishReason of the response.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. N-shots Prompting Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some prompting techniques that we can make use of to perform tasks more effectively with LLM. In this section, we will cover what is zero-shot, one-shot, and few-shots prompting. This is also known as N-shots prompting. While walking through these concepts, we will also introduce what are some real world applications of LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Zero Shot Prompting \n",
    "\n",
    "Zero shot prompting refers to prompting the model to generate predictions on unseen data without the need for any additional training. This can be used for more straightforward tasks as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Document Question & Answering\n",
    "LLMs are able to make inferences based on the context of the input text. This is done through a technique called \"attention,\" which allows the model to focus on specific parts of the input text when making predictions. For example, if you input a document about a particular topic, the model can infer the main points of the document and answer follow-up questions about the topic. This allows you to interact with and draw insights from input document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a branch. \\\n",
    "The fox devised a plan and flattered the crow, causing the crow to caw with delight, dropping the \\\n",
    "cheese which the fox quickly snatched up and ran away. The crow learned a valuable lesson and never trusted the fox again.\"\"\"\n",
    "\n",
    "question = 'Who got cheated?'\n",
    "question1 = 'How did the fox cheat the crow?'\n",
    "question2 = 'what is the lesson from this story?'\n",
    "\n",
    "'''\n",
    "Change the question variable here to test out the other questions. Feel free to ask your own questions and see what will the \n",
    "answer be!\n",
    "'''\n",
    "prompt = f'Context: {context}<br>Question: {question}<br>Answer:'\n",
    "#logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The crow\n"
     ]
    }
   ],
   "source": [
    "import json as json\n",
    "\n",
    "payload = {\"prompt\": prompt, \"maxTokens\": 100}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis/Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = 'I hated the movie. Thoroughly disappointing for a sequel.'\n",
    "# review = 'this product is fine so far'\n",
    "sentiment = 'Sentiment (Good, Bad)'\n",
    "\n",
    "prompt = f'Review: {review}\\n{sentiment}:'\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": 0.0, \"seed\": 321}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Inference (NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location = \"United States\"\n",
    "# location = \"United Kingdom\"\n",
    "prompt = f\"\"\"The world cup has kicked off in Los Angeles, United States.\n",
    "Based on the paragraph above can we conclude that: \"The world cup takes place in {location}\"?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, people can conclude that \"The world cup takes place in United States\" based on the paragraph above.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\":prompt, \"maxTokens\":100}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Title: \"University has new facility coming up\"\n",
    "Given the above title of an imaginary article, imagine the article.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 200}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Translate to German:  My name is Arthur\"\n",
    "\n",
    "logger.info(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 100, \"temperature\": 0.2, \"do_sample\": False, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Customer: Hi there, I'm having a problem with my iPhone.\n",
    "Agent: Hi! I'm sorry to hear that. What's happening?\n",
    "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. I've tried different charging cables and power adapters, but the issue persists.\n",
    "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery life?\n",
    "Customer: Yes, there are some apps that are using up a lot of battery.\n",
    "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then swiping up on the app to close it.\n",
    "Customer: I did that, but the issue is still there.\n",
    "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
    "Customer: Okay, I did that. What's next?\n",
    "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
    "Customer: Alright, I restarted it, but it's still not charging properly.\n",
    "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
    "Customer: Do I need to make an appointment?\n",
    "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Okay, will I have to pay for the repairs?\n",
    "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't have to pay anything. However, if it's not covered under warranty, you will have to pay for the repairs.\n",
    "Customer: How long will it take to get my iPhone back?\n",
    "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
    "Customer: Can I track the repair status online?\n",
    "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Alright, thanks for your help.\n",
    "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
    "Customer: No, that's all for now.\n",
    "Agent: Alright, have a great day and good luck with your iPhone!\n",
    "\"\"\"\n",
    "\n",
    "query = (\n",
    "    \"What are the customer and agent talking about?\"\n",
    ")\n",
    "\n",
    "# \"What troubleshooting steps were suggested to the customer to fix their iPhone charging issue?\"\n",
    "# \"Was resetting the iPhone to its default settings able to solve the charging issue and battery draining?\"\n",
    "# \"What steps can the customer take to make an appointment at the nearest Apple Store or authorized service provider?\"\n",
    "# \"What is the overall sentiment and sentiment score of the conversation between the customer and the agent?\"\n",
    "# \"identify any specific words, phrases, or context that influenced the {sentiment} sentiment.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 150, \"temperature\": 0.5, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Context Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_context = \"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) \\\n",
    "were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. \\\n",
    "They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates from Denmark, \\\n",
    "Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. \\\n",
    "Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, \\\n",
    "their descendants would gradually merge with the Carolingian-based cultures of West Francia. \\\n",
    "The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, \\\n",
    "and it continued to evolve over the succeeding centuries.\"\"\"\n",
    "test_question = \"In what country is Normandy located?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context: \n",
    "{test_context}\n",
    "\n",
    "{test_question}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 100, \"temperature\": 1.2, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. One-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_article = 'I love apples especially the large juicy ones. Apples are a great source of vitamins and fiber. An apple a day keeps the doctor away!'\n",
    "train_summary = 'I love apples.'\n",
    "\n",
    "test_article = 'I hate oranges especially the bitter ones. They are high in citric acid and they give me heart burns. Doctor suggests me to avoid them!'\n",
    "test_summary = 'I hate oranges.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "article: {train_article}\n",
    "summary: {train_summary}\n",
    "--\n",
    "article: {test_article}\n",
    "summary:\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": 0.5, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Generation (NLG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "train_out = 'The Punter provides Indian food in the cheap price range.'\n",
    "\n",
    "test_inp = 'name[Blue Spice], eatType[coffee shop], price_range[expensive]'\n",
    "test_out = 'Blue Spice is a coffee shop that is a bit expensive.'\n",
    "\n",
    "prompt = (\n",
    "    f\"{train_inp} ==> {train_out}\\n\"\n",
    "    f\"--\\n\"\n",
    "    f\"{test_inp} ==>\"\n",
    ")\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 100, \"temperature\": 0.5, \"seed\": 123, \"num_beams\": 3}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flip (Entity Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'The Punter provides Indian food in the cheap price range.'\n",
    "train_out = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "\n",
    "test_inp = 'Blue Spice is a coffee shop that is a bit pricy.'\n",
    "test_out = 'name[Blue Spice], eat_type[coffee shop], price_range[pricy]'\n",
    "\n",
    "prompt = (\n",
    "    f\"\"\"{train_inp} ==> {train_out}\\n\"\"\"\n",
    "    f\"--\\n\"\n",
    "    f\"\"\"{test_inp} ==>\"\"\"\n",
    ")\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 100, \"temperature\": 0.1, \"do_sample\": False, \"seed\": 123, \"num_beams\": 3}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Few-shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract main person:\n",
    "\n",
    "s: John is playing basketball\n",
    "p: John\n",
    "##\n",
    "s: Jeff and Phil are chatting about GAI. Phil has to run. He is in a rush\n",
    "p: Phil\n",
    "##\n",
    "s: Max is older than Emma\n",
    "p: Max\n",
    "##\n",
    "s: Susan misses the bus this morning but still get in time for her meeting with Sara\n",
    "p:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": .1, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Classify the topic of the following paragraph: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group, which has a reputation for making well-timed and occasionally controversial plays in the defense industry, has quietly placed its bets on another part of the market.\n",
    "Label: Business.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Some People Not Eligible to Get in on Google IPO Google has billed its IPO as a way for everyday people to get in on the process, denying Wall Street the usual stranglehold it's had on IPOs. Public bidding, a minimum of just five shares, an open process with 28 underwriters - all this pointed to a new level of public participation. But this isn't the case.\n",
    "Label: Technology.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Indians Mount Charge The Cleveland Indians pulled within one game of the AL Central lead by beating the Minnesota Twins, 7-1, Saturday night with home runs by Travis Hafner and Victor Martinez.\n",
    "Label: Sports.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Uptown girl, she's been living in her uptown world, I bet she never had a backstreet guy, I bet her mother never told her why, I'm gonna try.\n",
    "Label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": .1, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My native language is not English, I have blogs and I write my own articles. I also get articles from outsource writers, so I want to use it to re-write such articles, so i will create a tool for that\n",
    "NLP Task: Paraphrasing\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Just experimenting with content generation\n",
    "NLP task: Long form generation\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My company MetaDialog provides human in the loop automated support for costumers, we are currently using GPT3 to generate answers using our custom search engine to clients questions, and then provide the answers as suggestions to human agents to use or reject them as real answers to clients.\n",
    "NLP task: Conversational agent\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Receipt extraction including line items from plain text (sources being OCR-ed images, PDFs and HTML Emails)\n",
    "NLP task: Information extraction\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: I have a lot of legacy documentation which needs to be cleaned, summarized, and queried. I think AI21 would help.\n",
    "NLP task: Summarization\n",
    "\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Answer questions based on a given corpus of information\n",
    "NLP task: Question answering\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: creating useful content for companies websites articles\n",
    "NLP task:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": .1, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
