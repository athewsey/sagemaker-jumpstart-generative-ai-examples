{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Prompt Engineering\n",
    "\n",
    "***\n",
    "In this lab, we will cover what is prompt engineering, what are some real world use cases, and advanced prompt engineering insights.\n",
    "***\n",
    "\n",
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [N-shots Prompting Concepts](#2.-run-inference-on-the-pre-trained-model)\n",
    "3. [Chain of Thought (CoT) Prompting](#3.-Query-endpoint-and-parse-response)\n",
    "4. [Tree of Thought (ToT) Prompting](#4.-Use-Cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set Up\n",
    "\n",
    "The endpoint URL is pointing to a AI21 Jurassic 2 Grande Instruct model deployed on Amazon Sagemaker endpoint. This will be the model used for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "url = \"https://e165qoav65.execute-api.us-east-1.amazonaws.com/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a method that will invoke the REST API via HTTP request to interact with the model. The returned response will be the\n",
    "# text generated by the model.\n",
    "def query_endpoint_with_json_payload(url, payload):\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        json=payload,\n",
    "    )\n",
    "    \n",
    "    response_body = response.content\n",
    "    response_json = json.loads(response_body.decode('utf-8'))\n",
    "    generated_text = response_json['completions'][0]['data']['text']\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Parameters\n",
    "\n",
    "***\n",
    "AI21 Jurassic 2 Models supports many advanced parameters while performing inference. Here are some commonly used parameters:\n",
    "\n",
    "* **maxTokens:** The maximum number of tokens to generate per result. Optional, default = 16. If no stopSequences are given, generation is stopped after producing maxTokens\n",
    "* **numResults:** Number of completions to sample and return. Optional, default = 1\n",
    "* **temperature:** Modifies the distribution from which tokens are sampled. Optional, default = 0.7 Setting temperature to 1.0 samples directly from the model distribution. Lower (higher) values increase the chance of sampling higher (lower) probability tokens. A value of 0 essentially disables sampling and results in greedy decoding, where the most likely token is chosen at every step. â€‹\n",
    "* **topKReturn:** 0 <= integer <= 10, Optional, default = 0. Return the top-K alternative tokens. When using a non-zero value, the response includes the string representations and logprobs for each of the top-K alternatives at each position, in the prompt and in the completions.\n",
    "* **topP:** Sample tokens from the corresponding top percentile of probability mass. Optional, default = 1. For example, a value of 0.9 will only consider tokens comprising the top 90% probability mass.\n",
    "* **stopSequences:** Stops decoding if any of the strings is generated. Optional. For example, to stop at a comma or a new line use [\".\", \"\\n\"]. The decoded result text will not include the stop sequence string, but it will be included in the raw token data, which can also continue beyond the stop sequence if the sequence ended in the middle of a token. The sequence which triggered the termination will be included in the finishReason of the response.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. N-shots Prompting Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some prompting techniques that we can make use of to perform tasks more effectively with LLM. In this section, we will cover what is zero-shot, one-shot, and few-shots prompting. This is also known as N-shots prompting. While walking through these concepts, we will also introduce what are some real world applications of LLM.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Zero Shot Prompting \n",
    "\n",
    "Zero shot prompting refers to prompting the model to generate predictions on unseen data without the need for any additional training. This can be used for more straightforward tasks as shown below.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.1 In Context Question & Answering\n",
    "LLMs are able to make inferences based on the context of the input text. This is done through a technique called \"attention,\" which allows the model to focus on specific parts of the input text when making predictions. For example, if you input a document about a particular topic, the model can infer the main points of the document and answer follow-up questions about the topic. This allows you to interact with and draw insights from documents, transcripts, articles etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Document QnA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "Context: Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a branch. The fox devised a plan and flattered the crow, causing the crow to caw with delight, dropping the cheese which the fox quickly snatched up and ran away. The crow learned a valuable lesson and never trusted the fox again.<br>Question: How did the fox cheat the crow?<br>Answer:\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a branch. \\\n",
    "The fox devised a plan and flattered the crow, causing the crow to caw with delight, dropping the \\\n",
    "cheese which the fox quickly snatched up and ran away. The crow learned a valuable lesson and never trusted the fox again.\"\"\"\n",
    "\n",
    "question = 'Who got cheated?'\n",
    "question1 = 'How did the fox cheat the crow?'\n",
    "question2 = 'what is the lesson from this story?'\n",
    "\n",
    "'''\n",
    "Change the question variable here to test out the other questions. Feel free to ask your own questions and see what will the \n",
    "answer be!\n",
    "'''\n",
    "prompt = f'Context: {context}<br>Question: {question1}<br>Answer:'\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \n",
      "The original price of the gallon of milk is $3.99.\n",
      "The discount is 25%, so 3.99/100 * 25 = $0.99.\n",
      "Discounted price is $3.99-$0.99=$3.00.\n",
      "$1 off the entire purchase is $3.00-$1=$2.00.\n",
      "The final price of the gallon of milk after all the discount is $2.00.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Transcript QnA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "Context: \n",
      "\n",
      "Customer: Hi there, I'm having a problem with my iPhone.\n",
      "Agent: Hi! I'm sorry to hear that. What's happening?\n",
      "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. I've tried different charging cables and power adapters, but the issue persists.\n",
      "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery life?\n",
      "Customer: Yes, there are some apps that are using up a lot of battery.\n",
      "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then swiping up on the app to close it.\n",
      "Customer: I did that, but the issue is still there.\n",
      "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
      "Customer: Okay, I did that. What's next?\n",
      "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
      "Customer: Alright, I restarted it, but it's still not charging properly.\n",
      "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
      "Customer: Do I need to make an appointment?\n",
      "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
      "Customer: Okay, will I have to pay for the repairs?\n",
      "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't have to pay anything. However, if it's not covered under warranty, you will have to pay for the repairs.\n",
      "Customer: How long will it take to get my iPhone back?\n",
      "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
      "Customer: Can I track the repair status online?\n",
      "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized service provider.\n",
      "Customer: Alright, thanks for your help.\n",
      "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
      "Customer: No, that's all for now.\n",
      "Agent: Alright, have a great day and good luck with your iPhone!\n",
      "\n",
      "\n",
      "What are the customer and agent talking about?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Customer: Hi there, I'm having a problem with my iPhone.\n",
    "Agent: Hi! I'm sorry to hear that. What's happening?\n",
    "Customer: The phone is not charging properly, and the battery seems to be draining very quickly. I've tried different charging cables and power adapters, but the issue persists.\n",
    "Agent: Hmm, that's not good. Let's try some troubleshooting steps. Can you go to Settings, then Battery, and see if there are any apps that are using up a lot of battery life?\n",
    "Customer: Yes, there are some apps that are using up a lot of battery.\n",
    "Agent: Okay, try force quitting those apps by swiping up from the bottom of the screen and then swiping up on the app to close it.\n",
    "Customer: I did that, but the issue is still there.\n",
    "Agent: Alright, let's try resetting your iPhone's settings to their default values. This won't delete any of your data. Go to Settings, then General, then Reset, and then choose Reset All Settings.\n",
    "Customer: Okay, I did that. What's next?\n",
    "Agent: Now, let's try restarting your iPhone. Press and hold the power button until you see the \"slide to power off\" option. Slide to power off, wait a few seconds, and then turn your iPhone back on.\n",
    "Customer: Alright, I restarted it, but it's still not charging properly.\n",
    "Agent: I see. It looks like we need to run a diagnostic test on your iPhone. Please visit the nearest Apple Store or authorized service provider to get your iPhone checked out.\n",
    "Customer: Do I need to make an appointment?\n",
    "Agent: Yes, it's always best to make an appointment beforehand so you don't have to wait in line. You can make an appointment online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Okay, will I have to pay for the repairs?\n",
    "Agent: That depends on whether your iPhone is covered under warranty or not. If it is, you won't have to pay anything. However, if it's not covered under warranty, you will have to pay for the repairs.\n",
    "Customer: How long will it take to get my iPhone back?\n",
    "Agent: It depends on the severity of the issue, but it usually takes 1-2 business days.\n",
    "Customer: Can I track the repair status online?\n",
    "Agent: Yes, you can track the repair status online or by calling the Apple Store or authorized service provider.\n",
    "Customer: Alright, thanks for your help.\n",
    "Agent: No problem, happy to help. Is there anything else I can assist you with?\n",
    "Customer: No, that's all for now.\n",
    "Agent: Alright, have a great day and good luck with your iPhone!\n",
    "\"\"\"\n",
    "\n",
    "query = (\n",
    "    \"What are the customer and agent talking about?\"\n",
    ")\n",
    "\n",
    "# \"What troubleshooting steps were suggested to the customer to fix their iPhone charging issue?\"\n",
    "# \"Was resetting the iPhone to its default settings able to solve the charging issue and battery draining?\"\n",
    "# \"What steps can the customer take to make an appointment at the nearest Apple Store or authorized service provider?\"\n",
    "# \"What is the overall sentiment and sentiment score of the conversation between the customer and the agent?\"\n",
    "# \"identify any specific words, phrases, or context that influenced the {sentiment} sentiment.\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The customer is having a problem with their iPhone, and the agent is helping them troubleshoot the issue. The customer is not charging properly, and the battery is draining quickly. The agent suggests force quitting some apps, resetting the iPhone's settings, restarting it, and running diagnostic tests. The customer may have to pay for the repairs if the iPhone is not under warranty.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "***\n",
    "#### 2.1.2 Multi-Class Classification\n",
    "\n",
    "LLMs have natural language understanding capabilities. Given an input, they are able to infer the meaning of the texts and classify them into the categories you define. Below are some possible uses cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Sentiment Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "Analyze the sentiment of the following review:\n",
      "I hated the movie. Thoroughly disappointing for a sequel.\n",
      "Classify the sentiment as either one of the following [positive, neutral, negative].\n",
      "Sentiment:\n"
     ]
    }
   ],
   "source": [
    "review = 'I hated the movie. Thoroughly disappointing for a sequel.'\n",
    "review1 = 'The movie is alright. Will consider watching it again'\n",
    "review2 = 'What an awesome movie! Excited for the next sequel to be out.'\n",
    "\n",
    "prompt = f'Analyze the sentiment of the following review:\\n{review}\\nClassify the sentiment as either one of the following [positive, neutral, negative].\\nSentiment:'\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " negative\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "##### *Topic Modelling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "Analyze the following article:\n",
      "A car is a four-wheeled motor vehicle that is designed to carry passengers. Cars are powered by an internal combustion engine and can travel at speeds of up to 100 miles per hour. Cars are a popular mode of transportation for both personal and commercial use.\n",
      "Classify the article as either one of the following topics [Animal, Food, Vehicle].\n",
      "Topic:\n"
     ]
    }
   ],
   "source": [
    "article = 'Penguins are flightless birds that live in the Southern Hemisphere. They are excellent swimmers and divers, and their diet consists of fish, squid, and krill. Penguins are social animals, and they live in colonies. They are known for their waddling gait and their black and white feathers, which help them to camouflage themselves from predators.'\n",
    "article1 = 'Laksa is a spicy noodle soup popular in Southeast Asia. It is typically made with a rich and flavorful coconut milk broth, rice noodles, seafood, and a variety of herbs and spices. Laksa is a hearty and satisfying dish that is sure to warm you up on a cold day.'\n",
    "article2 = 'A car is a four-wheeled motor vehicle that is designed to carry passengers. Cars are powered by an internal combustion engine and can travel at speeds of up to 100 miles per hour. Cars are a popular mode of transportation for both personal and commercial use.'\n",
    "\n",
    "prompt = f'Analyze the following article:\\n{article2}\\nClassify the article as either one of the following topics [Animal, Food, Vehicle].\\nTopic:'\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Vehicle\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.1.3 Natural Language Inference (NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: The world cup has kicked off in Los Angeles, United States.\n",
      "Based on the paragraph above can we conclude that: \"The world cup takes place in United States\"?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location = \"United States\"\n",
    "# location = \"United Kingdom\"\n",
    "prompt = f\"\"\"The world cup has kicked off in Los Angeles, United States.\n",
    "Based on the paragraph above can we conclude that: \"The world cup takes place in {location}\"?\n",
    "\"\"\"\n",
    "logger.info(f\"Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, it is not possible to conclude from the paragraph that the world cup takes place in United States. The paragraph only mentions that the world cup has kicked off in Los Angeles, United States. It does not provide any information about the location or host country of the world cup.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "#### 2.1.4 Text Generation\n",
    "The ability of LLMs to generate text is one of its greatest strength. This is an area that is under continuous research. Based on the many experiements and research in the current landscape, here are some tips and ideas on how you can leverage this capability on real world use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### *Generating a product description*\n",
    "When generatiing creative write-ups like product description, setting a higher temperature gives the LLM some space to be creative. This way, you can generate multiple outputs and choose 1 that suits you the most! The more details you provide, the more accurate the generated description will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "Product features:\n",
      "- A bottle that can keep your hot drinks hot for up to 12 hours, cold drinks cold for up to 8 hours\n",
      "- Small and can fit into most bags easily, making it easy to carry around\n",
      "- Modern woody designs, making it aesthetically pleasing\n",
      "\n",
      "Based on the above product features, help to write a product description that will attract consumers to purchase the bottle. Include some marketing gimmicks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Product features:\n",
    "- A bottle that can keep your hot drinks hot for up to 12 hours, cold drinks cold for up to 8 hours\n",
    "- Small and can fit into most bags easily, making it easy to carry around\n",
    "- Modern woody designs, making it aesthetically pleasing\n",
    "\n",
    "Based on the above product features, help to write a product description that will attract consumers to purchase the bottle. Include some marketing gimmicks.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This bottle is perfect for those who want to enjoy a hot drink on the go. It's small and can fit in most bags, so you can easily take it with you. The modern design is also aesthetically pleasing. Plus, the insulation will keep your drinks hot for up to 12 hours or cold for up to 8 hours.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0.7}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "&nbsp;\n",
    "##### *Editing texts to different styles*\n",
    "LLMs are trained on large corpus of texts and have the ability to generate texts in writing styles. Here, we explore the following styles that LLM are able to generate for us:\n",
    "* Changing the type of narrative\n",
    "* Adapting to different domain\n",
    "* Constructing an email\n",
    "\n",
    "There are many more possible styles that it can generate, feel free to give it any style you can think of!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "Write-up:\n",
      "Singapore is a city-state located in Southeast Asia. It is a small country, only about 710 square kilometers in size, but it is very densely populated. The population of Singapore is about 5 million people, and it is made up of people from many different cultures, including Chinese, Malay, Indian, and Eurasian.\n",
      "\n",
      "Re-write the above write-up as a business formal email.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "style = 'first person narrative'\n",
    "style1 = 'travel agency marketing campaign'\n",
    "style2 = 'business formal email'\n",
    "prompt = f\"\"\"Write-up:\n",
    "Singapore is a city-state located in Southeast Asia. It is a small country, only about 710 square kilometers in size, but it is very densely populated. The population of Singapore is about 5 million people, and it is made up of people from many different cultures, including Chinese, Malay, Indian, and Eurasian.\n",
    "\n",
    "Re-write the above write-up as a {style}.\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dear Sir/Madam,\n",
      "\n",
      "I am writing to you today to inform you about Singapore. Singapore is a city-state located in Southeast Asia. It is a small country, only about 710 square kilometers in size, but it is very densely populated. The population of Singapore is about 5 million people, and it is made up of people from many different cultures, including Chinese, Malay, Indian, and Eurasian.\n",
      "\n",
      "Singapore is known for its clean and efficient government, as well as its strong economy. The country has a highly developed infrastructure, and is known for its excellent public transportation system. Singapore is also known for its many tourist attractions, including the famous Singapore Botanic Gardens, the Singapore Zoo, and the Singapore River Cruise.\n",
      "\n",
      "If you are interested in visiting Singapore, I would be happy to provide you with more information. I look forward to hearing from you soon.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.5 Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Translate to German:  My name is Arthur\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 100, \"temperature\": 0.2, \"do_sample\": False, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### In Context Q&A\n",
    "Move to document QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_context = \"\"\"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) \\\n",
    "were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. \\\n",
    "They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates from Denmark, \\\n",
    "Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. \\\n",
    "Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, \\\n",
    "their descendants would gradually merge with the Carolingian-based cultures of West Francia. \\\n",
    "The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, \\\n",
    "and it continued to evolve over the succeeding centuries.\"\"\"\n",
    "test_question = \"In what country is Normandy located?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context: \n",
    "{test_context}\n",
    "\n",
    "{test_question}\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 100, \"temperature\": 1.2, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 One-shot / Few shot Learning\n",
    "Few shot learning is useful when you want the generated text to follow specific structures, tone, or choice of words. It works by providing a one or few examples within the prompt. The LLM will be able to learn from the examples and generate similar output. This is also known as In-Context Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Text Summarisation\n",
    "Move to zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_article = 'I love apples especially the large juicy ones. Apples are a great source of vitamins and fiber. An apple a day keeps the doctor away!'\n",
    "train_summary = 'I love apples.'\n",
    "\n",
    "test_article = 'I hate oranges especially the bitter ones. They are high in citric acid and they give me heart burns. Doctor suggests me to avoid them!'\n",
    "test_summary = 'I hate oranges.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "article: {train_article}\n",
    "summary: {train_summary}\n",
    "--\n",
    "article: {test_article}\n",
    "summary:\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": 0.5, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Natural Language Generation (NLG)\n",
    "Move to text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "train_out = 'The Punter provides Indian food in the cheap price range.'\n",
    "\n",
    "test_inp = 'name[Blue Spice], eatType[coffee shop], price_range[expensive]'\n",
    "test_out = 'Blue Spice is a coffee shop that is a bit expensive.'\n",
    "\n",
    "prompt = (\n",
    "    f\"{train_inp} ==> {train_out}\\n\"\n",
    "    f\"--\\n\"\n",
    "    f\"{test_inp} ==>\"\n",
    ")\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 100, \"temperature\": 0.5, \"seed\": 123, \"num_beams\": 3}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Entity Extraction\n",
    "We can perform entity extraction by providing examples to the LLM. The LLM will follow the stucture in the provided examples when generating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: The Punter provides Indian food in the cheap price range. ==> name[The Punter], eat_type[Indian], price_range[cheap]\n",
      "--\n",
      "Blue Spice is a coffee shop that is a bit pricy. ==>\n"
     ]
    }
   ],
   "source": [
    "train_inp = 'The Punter provides Indian food in the cheap price range.'\n",
    "train_out = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "\n",
    "test_inp = 'Blue Spice is a coffee shop that is a bit pricy.'\n",
    "test_out = 'name[Blue Spice], eat_type[coffee shop], price_range[pricy]'\n",
    "\n",
    "prompt = (\n",
    "    f\"\"\"{train_inp} ==> {train_out}\\n\"\"\"\n",
    "    f\"--\\n\"\n",
    "    f\"\"\"{test_inp} ==>\"\"\"\n",
    ")\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " name[Blue Spice], eat_type[coffee shop], price_range[pricy]\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "#### 2.2.2 Natural Language Query\n",
    "Given information on the database schema, LLMs can generate the SQL query required to extract the information you have requested for. Here is an example on how the LLM can generate SQL queries based on a question asked in natural language. A few examples is also shown to ensure that the generated output follows a fixed structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "Create SQL statement from instruction.\n",
      "\n",
      "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
      "Request: all the countries we have customers in without repetitions.\n",
      "SQL statement:\n",
      "SELECT DISTINCT Country FROM Customers;\n",
      "\n",
      "##\n",
      "\n",
      "Create SQL statement from instruction.\n",
      "\n",
      "Database: Products(ProductID, ProductName, SupplierID, CategoryID, Unit, Price)\n",
      "Request: selects all products from categories 1 and 7\n",
      "SQL statement:\n",
      "SELECT * FROM Products\n",
      "WHERE CategoryID = 1 OR CategoryID = 7;\n",
      "\n",
      "##\n",
      "\n",
      "Create SQL statement from instruction.\n",
      "\n",
      "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
      "Request: there is a customer who lives in Frankfurt city. change the customer's name to Alfred Schmidt.\n",
      "SQL statement:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Create SQL statement from instruction.\n",
    "\n",
    "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
    "Request: all the countries we have customers in without repetitions.\n",
    "SQL statement:\n",
    "SELECT DISTINCT Country FROM Customers;\n",
    "\n",
    "##\n",
    "\n",
    "Create SQL statement from instruction.\n",
    "\n",
    "Database: Products(ProductID, ProductName, SupplierID, CategoryID, Unit, Price)\n",
    "Request: selects all products from categories 1 and 7\n",
    "SQL statement:\n",
    "SELECT * FROM Products\n",
    "WHERE CategoryID = 1 OR CategoryID = 7;\n",
    "\n",
    "##\n",
    "\n",
    "Create SQL statement from instruction.\n",
    "\n",
    "Database: Customers(CustomerID, CustomerName, ContactName, Address, City, PostalCode, Country)\n",
    "Request: there is a customer who lives in Frankfurt city. change the customer's name to Alfred Schmidt.\n",
    "SQL statement:\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "UPDATE Customers\n",
      "SET CustomerName = 'Alfred Schmidt'\n",
      "WHERE City = 'Frankfurt';\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 200, \"temperature\": 0, \"stopSequences\": [\"##\"]}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract main person:\n",
    "\n",
    "s: John is playing basketball\n",
    "p: John\n",
    "##\n",
    "s: Jeff and Phil are chatting about GAI. Phil has to run. He is in a rush\n",
    "p: Phil\n",
    "##\n",
    "s: Max is older than Emma\n",
    "p: Max\n",
    "##\n",
    "s: Susan misses the bus this morning but still get in time for her meeting with Sara\n",
    "p:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": .1, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Classify the topic of the following paragraph: Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group, which has a reputation for making well-timed and occasionally controversial plays in the defense industry, has quietly placed its bets on another part of the market.\n",
    "Label: Business.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Some People Not Eligible to Get in on Google IPO Google has billed its IPO as a way for everyday people to get in on the process, denying Wall Street the usual stranglehold it's had on IPOs. Public bidding, a minimum of just five shares, an open process with 28 underwriters - all this pointed to a new level of public participation. But this isn't the case.\n",
    "Label: Technology.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Indians Mount Charge The Cleveland Indians pulled within one game of the AL Central lead by beating the Minnesota Twins, 7-1, Saturday night with home runs by Travis Hafner and Victor Martinez.\n",
    "Label: Sports.\n",
    "\n",
    "##\n",
    "\n",
    "Classify the topic of the following paragraph: Uptown girl, she's been living in her uptown world, I bet she never had a backstreet guy, I bet her mother never told her why, I'm gonna try.\n",
    "Label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": .1, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My native language is not English, I have blogs and I write my own articles. I also get articles from outsource writers, so I want to use it to re-write such articles, so i will create a tool for that\n",
    "NLP Task: Paraphrasing\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Just experimenting with content generation\n",
    "NLP task: Long form generation\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: My company MetaDialog provides human in the loop automated support for costumers, we are currently using GPT3 to generate answers using our custom search engine to clients questions, and then provide the answers as suggestions to human agents to use or reject them as real answers to clients.\n",
    "NLP task: Conversational agent\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Receipt extraction including line items from plain text (sources being OCR-ed images, PDFs and HTML Emails)\n",
    "NLP task: Information extraction\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: I have a lot of legacy documentation which needs to be cleaned, summarized, and queried. I think AI21 would help.\n",
    "NLP task: Summarization\n",
    "\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: Answer questions based on a given corpus of information\n",
    "NLP task: Question answering\n",
    "##\n",
    "\n",
    "Classify the below use-case description to one of the following NLP tasks: Short form generation, Long form generation, Summarization, Classification, Question answering, Paraphrasing, Conversational agent, Information extraction, Generate code\n",
    "\n",
    "Use case: creating useful content for companies websites articles\n",
    "NLP task:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"text_inputs\":prompt, \"max_length\": 50, \"temperature\": .1, \"do_sample\": True, \"seed\": 123}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "response = parse_response_multiple_texts(response)[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Fraud Detection\n",
    "Provide a few sample emails that are legitimate and fraudulent. The LLM will learn from the examples and use the information to determine if an email is fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Chain of Thought (CoT) Prompting\n",
    "Chain of thought prompting is a method for improving the reasoning abilities of large language models. It works by providing the model with a few examples of how to solve a problem step-by-step. This helps the model to learn how to break down complex problems into smaller, more manageable tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "Question: You are shopping at a store and you see a shirt that you like. The shirt is originally priced at $20, but it is on sale for 20% off. You also have a coupon that get you $4 off of your entire purchase. How much will you pay for the shirt with all the discounts? Solve the problem step by step.\n",
      "Answer: \n",
      "The original price of the shirt is $20.\n",
      "The discount is 20%, so 20/100 * 20 = $4.\n",
      "Discounted price is $20-$4=$16.\n",
      "$4 off the entire purchase is $16-$4=$12.\n",
      "The final price of the shirt after all the discount is $12.\n",
      "\n",
      "Question: You are shopping at a grocery store and you see a gallon of milk that is originally priced at $3.99. The milk is on sale for 25% off. You also have a coupon for $1 off of your entire purchase. How much will you pay for the milk? Solve the problem step by step.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Question: You are shopping at a store and you see a shirt that you like. The shirt is originally priced at $20, but it is on sale for 20% off. You also have a coupon that get you $4 off of your entire purchase. How much will you pay for the shirt with all the discounts? Solve the problem step by step.\n",
    "Answer: \n",
    "The original price of the shirt is $20.\n",
    "The discount is 20%, so 20/100 * 20 = $4.\n",
    "Discounted price is $20-$4=$16.\n",
    "$4 off the entire purchase is $16-$4=$12.\n",
    "The final price of the shirt after all the discount is $12.\n",
    "\n",
    "Question: You are shopping at a grocery store and you see a gallon of milk that is originally priced at $3.99. The milk is on sale for 25% off. You also have a coupon for $1 off of your entire purchase. How much will you pay for the milk? Solve the problem step by step.\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \n",
      "The original price of the gallon of milk is $3.99.\n",
      "The discount is 25%, so 3.99/100 * 25 = $0.99.\n",
      "Discounted price is $3.99-$0.99=$3.00.\n",
      "$1 off the entire purchase is $3.00-$1=$2.00.\n",
      "The final price of the gallon of milk after all the discount is $2.00.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 300, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have looked at how LLMs can learn from In-Context examples and perform multi-step reasoning to solve a mathematical problem. Here, we look at how we can explicitly instruct the LLM to solve the problem 'step by step' to improve its logical reasoning capability and derive the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[4mPrompt\u001b[0m\n",
      "You are planning a party and you need to buy food and drinks for 20 people. Each person requires 3 cup of drinks, and 3 slices of pizza. 1 cup of drink cost $2. 1 slice of pizza cosr $3. How much will the total food and drinks cost for the party?\n",
      "Solve this problem step by step.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are planning a party and you need to buy food and drinks for 20 people. Each person requires 3 cup of drinks, and 3 slices of pizza. 1 cup of drink cost $2. 1 slice of pizza cosr $3. How much will the total food and drinks cost for the party?\n",
    "Solve this problem step by step.\"\"\"\n",
    "\n",
    "logger.info(f\"\\u001b[4mPrompt\\u001b[0m\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1. First find the cost of 3 cups of drinks: 3 x $2 = $6\n",
      "2. Then find the cost of 3 slices of pizza: 3 x $3 = $9\n",
      "3. Then add the cost of 3 cups of drinks and 3 slices of pizza to find the total cost: $6 + $9 = $15\n",
      "4. Then multiply the total cost by 20 to find the total cost for 20 people: $15 x 20 = $300\n",
      "\n",
      "Therefore, the total cost for food and drinks for 20 people is $300.\n"
     ]
    }
   ],
   "source": [
    "payload = {\"prompt\": prompt, \"maxTokens\": 300, \"temperature\": 0}\n",
    "response = query_endpoint_with_json_payload(url, payload)\n",
    "logger.info(f'{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tree of Thoughts (ToT) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
